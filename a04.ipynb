{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unconstrained Optimization methods\n",
    "## Prepared by Maxim Khagay\n",
    "### School of Science and Technology, Nazarbayev University"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "This report will demonstrate two ways to solve optimization problem of two dimentional function $f(x, y)$. In our case we need to find minimum of function. These methods are steepest descent method and random search method. For random search method we will see how the error of this method scales with the number of iterations. Computational cost of each method will be considered. Finally, both methods will be run again at fixed $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Both methods are useful to find optimaztion solution of function. They use different methods:\n",
    "### Theory\n",
    "Two methods are described in Chapra and Canale's book.\n",
    "Let's briefly introduce them\n",
    "#### Steepest search method\n",
    "First of all, for steepest descent method we use iterations to improve our $x$ and $y$ with a condition until we reach acceptable precision. In each iteration we will calculate optimal $h_{opt}$ at which $g(h)$ is minimal by using gradient of function. After that we use $h_{opt}$ to take new values of $x$ and $y$. Actually to find $h_{opt}$ we use golden section method, it is covered in Chapra and Canale's book. Also as it can be seen in the book we derive error the following way:\n",
    "\n",
    "\\begin{equation*}\n",
    "R = \\frac{(\\sqrt5 - 1)}{2}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "error = (1-R)\\times|\\frac{x_u-x_l}{h_{opt}}|\\times100%\\n\n",
    "\\end{equation*}\n",
    "\n",
    "#### Random search method\n",
    "This method just calculates function of new values(generated randomly) and improve $x$ and $y$ if new values are better. Because of using random it is not easy to predict behavior of this method.\n",
    "\n",
    "To find computational cost we need to find time of each method. We will try to run each method 10 times and take total time and average from it. The main purpose to do it is other process of PC which can affect on process of method and by using average we can compare computational cost, because computational cost directly depends on number of executed iteration in code and number executed iterations is actually a time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formulation of the problem\n",
    "We need to find minimal optimization of function $f(x,y)=(x-3)^2+(y-2)^2$\n",
    "## Theoretical expectations\n",
    "It is simple function and we can derive that:\n",
    "\n",
    "First derivative for $x$ => 2(x-3) = 0, x = 3\n",
    "\n",
    "First derivative for $y$ => 2(y-2) = 0, y = 2\n",
    "\n",
    "Both second derivatives of $x$ and $y$ are 2, so we found minimum of function in $f(3, 2) = 0$\n",
    "\n",
    "Now we can expect from practice that $x = 3$ and $y = 2$ \n",
    "\n",
    "Also we can predict that one iteration is enough to reach from any points of initial $x$ and $y$ because function is paraboloid for steepest search method from:\n",
    "$g(h) = f(x_0 + 2(x_0-3)h,y_0+2(y_0-2)h)$\n",
    "\n",
    "$g'(h) = (x_0-3)^2(1+2h)^2+(y_0-2)^2(1+2h)^2 = f(x_0,y_0)(1+2h)^2$\n",
    "from that we know that it is positive and we found $h = -\\frac{1}{2}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we introduce our function $f(x, y)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from time import time\n",
    "\n",
    "def f(x, y):\n",
    "    return (x-3)**2+(y-2)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steepest search method is introduced here. We took initial values of $x$ and $y$ equal to 0. Also our function of gradient $g(h)$, we use $golden(xl, xu)$ to return a $h_{opt}$ with error precision $10^{-10}$, we use error from theory part. As we know that theoreticaly is $-\\frac{1}{2}$ we take range for golden section method from -1 to 1. And we print obtained values of $x$ and $y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9999999999974007 1.9999999999982672\n"
     ]
    }
   ],
   "source": [
    "def g(h):\n",
    "    return f(x0+2*(x0-3)*h,y0+2*(y0-2)*h)\n",
    "    \n",
    "def golden(xl, xu):\n",
    "    R = (sqrt(5)-1)/2\n",
    "    e = (1-R)*abs((xu-xl)/(min(xl, xu)))\n",
    "    while(e > 1e-10):\n",
    "        d = R*(xu-xl)\n",
    "        x1 = xl + d\n",
    "        x2 = xu - d\n",
    "        if (g(x1) < g(x2)):\n",
    "            xl = x2\n",
    "            x2 = x1\n",
    "            x1 = xl + R*(xu-xl)\n",
    "        else:\n",
    "            xu = x1\n",
    "            x1 = x2\n",
    "            x2 = xu - R*(xu-xl)\n",
    "        e = (1-R)*abs((xu-xl)/(min(xl, xu)))*100\n",
    "        \n",
    "    if (g(xl) < g(xu)):\n",
    "        return xl\n",
    "    else:\n",
    "        return xu\n",
    "\n",
    "x0 = 0\n",
    "y0 = 0\n",
    "\n",
    "while(f(x0, y0) > 1e-10):\n",
    "    h = golden(-1, 1)\n",
    "    x0 = x0 + 2*(x0-3)*h\n",
    "    y0 = y0 + 2*(y0-2)*h\n",
    "print(x0, y0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we introduce random search method. We put precision equals to $10^{-5}$. We use $mn$ as our minimum of function and recalculate values of $x$ and $y$. Code used Merssene Twisted algorithm to generate fixed random values. We get values of $x$ and $y$ in range from -5 to 5. Finally we print obtained values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.997304687334039 1.9996029904577908\n"
     ]
    }
   ],
   "source": [
    "from random import random, seed\n",
    "mn = 100000\n",
    "xl = yl = -5\n",
    "xu = yu = 5\n",
    "xmin = ymin = 0\n",
    "while(f(xmin,ymin) > 1e-5):\n",
    "    x = xl + (xu-xl)*random()\n",
    "    y = yl + (yu-yl)*random()\n",
    "    fnew = f(x,y)\n",
    "    if (fnew < mn):\n",
    "        xmin = x\n",
    "        ymin = y\n",
    "        mn = fnew\n",
    "print(xmin, ymin)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of code demonstrates 10000 iterations in loop and for each \"good\" iteration we save $error$. Finally we plot a graph and we can see behavior of $error$. We used $seed(1)$ to take fixed random values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9890545889386564 1.999355167316617\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEPCAYAAACgFqixAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8P8Bk3AGJasy3RYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x606a308cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last good iteration on 9010\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib.pyplot import *\n",
    "\n",
    "seed(1)\n",
    "n = np.array(np.zeros(100))\n",
    "r = np.array(np.zeros(100))\n",
    "xl = yl = -5\n",
    "xu = yu = 5\n",
    "xmin = ymin = 0\n",
    "i = -1\n",
    "mn = 10000\n",
    "for j in range(10000):\n",
    "    x = xl + (xu-xl)*random()\n",
    "    y = yl + (yu-yl)*random()\n",
    "    fnew = f(x,y)\n",
    "    if (fnew < mn):\n",
    "        xmin = x\n",
    "        ymin = y\n",
    "        mn = fnew\n",
    "        i += 1\n",
    "        n[i] = i\n",
    "        r[i] = fnew\n",
    "        xx = j\n",
    "print(xmin, ymin)\n",
    "xlabel(\"Iteration number\"); ylabel(\"Error\")\n",
    "plot(n,r)\n",
    "show()\n",
    "\n",
    "print(\"last good iteration on\",xx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we calculated average time on each steepest search method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00030026435852050783\n"
     ]
    }
   ],
   "source": [
    "avg1 = 0\n",
    "for t in range(10):\n",
    "    start = time()\n",
    "    x0 = 0\n",
    "    y0 = 0\n",
    "    while(f(x0, y0) > 1e-5):\n",
    "        h = golden(-10, 10)\n",
    "        x0 = x0 + 2*(x0-3)*h\n",
    "        y0 = y0 + 2*(y0-2)*h\n",
    "    end = time()\n",
    "    avg1 += end-start\n",
    "avg1 /= 10\n",
    "print(avg1)\n",
    "aa = avg1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Here we calculated average time on each random search method. As you see, we put $seed()$, it means that now we don't have fixed random values. We need a few seconds. Please wait."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.527218246459961\n"
     ]
    }
   ],
   "source": [
    "avg2 = 0\n",
    "seed()\n",
    "for t in range(10):\n",
    "    start = time()\n",
    "    mn = 100000\n",
    "    xl = yl = -5\n",
    "    xu = yu = 5\n",
    "    xmin = ymin = 0\n",
    "    while(f(xmin,ymin) > 1e-5):\n",
    "        x = xl + (xu-xl)*random()\n",
    "        y = yl + (yu-yl)*random()\n",
    "        fnew = f(x,y)\n",
    "        if (fnew < mn):\n",
    "            xmin = x\n",
    "            ymin = y\n",
    "            mn = fnew\n",
    "    end = time()\n",
    "    avg2 += end-start\n",
    "avg2 /= 10\n",
    "print(avg2)\n",
    "bb = avg2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In next two parts of code, we fix $y$ at 2 and search only $x$ and recalculate average time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0003984689712524414\n"
     ]
    }
   ],
   "source": [
    "avg1 = 0\n",
    "for t in range(10):\n",
    "    start = time()\n",
    "    x0 = 0\n",
    "    y0 = 2\n",
    "    while(f(x0, y0) > 1e-5):\n",
    "        h = golden(-10, 10)\n",
    "        x0 = x0 + 2*(x0-3)*h\n",
    "    end = time()\n",
    "    avg1 += end-start\n",
    "avg1 /= 10\n",
    "print(avg1)\n",
    "aaa = avg1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0020009994506835936\n"
     ]
    }
   ],
   "source": [
    "avg2 = 0\n",
    "seed()\n",
    "for t in range(10):\n",
    "    start = time()\n",
    "    mn = 100000\n",
    "    xl = -5\n",
    "    xu = 5\n",
    "    xmin = 0\n",
    "    ymin = 2\n",
    "    while(f(xmin,ymin) > 1e-5):\n",
    "        x = xl + (xu-xl)*random()\n",
    "        fnew = f(x,ymin)\n",
    "        if (fnew < mn):\n",
    "            xmin = x\n",
    "            mn = fnew\n",
    "    end = time()\n",
    "    avg2 += end-start\n",
    "avg2 /= 10\n",
    "print(avg2)\n",
    "bbb = avg2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "First of all, two methods work correctly. Steepest and random search methods give a correct values of $x$ and $y$, they are very close to true values $x=3$ and $y=2$ and satisfy to our precision in each method. \n",
    "\n",
    "From Error vs Iteration graph we see that the error converges to zero. Let me remind that our true value of optimal $f(x,y)$ equals to 0. So $Error = f(x_0,y_0) - true value$, so $Error = f(x_0, y_0)$. We see that 12 \"good\" iterations out of 10000 iterations were enough to reach true value with a given precision.\n",
    "\n",
    "Let's compare computational cost of two methods. As we said in theory part, we use average time of each method. Steepest search method is faster than random search method, we know that one iteration was enough for our simple function and because of it we get this result. Actually for random seach method time depends on our range, if we take more closer range time will decrease, accordingly computational cost will decrease. \n",
    "\n",
    "Below you see how much Steepest method is faster than Random method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31729.43433380975\n"
     ]
    }
   ],
   "source": [
    "print(bb/aa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we use fixed $y = 2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference between fixed y and non-fixed in time of Steepest method's time is  9.820461273193356e-05\n",
      "Proportion between fixed y and non-fixed in of Random method's time is  4761.229816032791 times\n"
     ]
    }
   ],
   "source": [
    "print(\"Difference between fixed y and non-fixed in time of Steepest method's time is \", aaa-aa)\n",
    "print(\"Proportion between fixed y and non-fixed in of Random method's time is \", bb/bbb, \"times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did not predict difference of Steepest method, is it positive or not? It depends on PC processes. However we can run many times Random method and see that after fixed $y$ computational cost will decrease, because we don't need calculate $y$ parameter.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "As we see from code results, two methods are reliable to find optimal solution of function. As we said our function is simple for steepest method, so it doesn't matter for computational cost of this method, in both cases of $y$ is almost the same. Whereas we can't predict behavior of random search method, we got that for fixed $y$ it takes less time. Let's notice that there are not a lot of  \"good\" iterations in random method, while we spend a huge amount of needless iterations, where our values didn't improve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
